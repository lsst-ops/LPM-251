
\section{Introduction}

All access to LSST proprietary data products will be through a Data Access Center (DAC). The United States's DAC will be hosted at the National Center for Supercomputing Applications (NCSA), where authorized users will perform scientific queries and analysis on the full data releases using the LSST Science Platform (LSP). The LSP is now well documented with the vision given in \citeds{LSE-319}, with more formal requirements in \citeds{LDM-554} and the design in \citeds{LDM-542}. The Chilean DAC will be equivalent in functionality to the US DAC, but scaled-down in terms of the computational resources available for query and analysis given the smaller Chilean community \citedsp{LDM-572}. All access to LSST proprietary data and data products is subject to the policies described in \citeds{LSE-349}.

Several LSST partner institutions have expressed interest in hosting the LSST data, in whole or in part, for their affiliated members as an independent Data Access Center (iDAC). In this document we lay out some ideas about what types of data products could be hosted, and the requirements and responsibilities that would be expected of an iDAC hosting LSST proprietary data products.


\section{Types of Data Products}

The full collection of LSST data products, from images to catalogs, is described by the Data Products Definitions Document \citedsp{LSE-163}. Below we describe three potential levels of data products that iDACs might consider hosting: the full data release with images, the data release catalogs, and a low-volume subset of the catalogs.

\subsection{Full Release(s)}

In this case the iDAC would be hosting all of the raw and processed images and catalogs, as described in \citedsp{LSE-163}. Including the raw image data in an iDAC requires roughly $6$ petabytes per year of survey, so this is a serious augmentation on resources in terms of both hardware and personnel. The processed data and associated calibrations bring the total data volume to $0.5$ exabytes for a single data release. Some data volume could be saved by taking only a single calibrated image per band, but the total would still be $60$ petabytes (with compression it may be possible to reduce this even further). Any iDAC considering hosting the full data release should also deploy the full LSST Science Platform \citeds{LSE-319} in order to maximize science productivity and their return on investment in hosting an iDAC. 

\subsection{Catalog Server}

Alternatively, an iDAC may find that hosting only the data release catalogs is sufficient for the scientific needs of its community. This will probably require the specific LSST database server \citedsp{LDM-135} and specific machines, and the deployment of the database system and associated subset of data access services (DAX; e.g., web APIs, Qserv, {\bf is there a good reference document for more DAX information?}). The full {\tt Object} catalog, which contains one row per object with a volume of $\approx 20$ kilobytes per row, is estimated to contain about $40 \times 10^9$ objects (even in the first full-sky data release). Adding to this the full {\tt Source} and {\tt Forced Source} catalogs, which contains one row per measurement in each of the $\sim80$ visit images obtained per year, brings the total storage volume required up into the petabytes range, and will require a serious commitment of resources at the proposed iDAC. The evolution of catalog sizes over the 10-year LSST survey is depicted in \figref{fig:catvol}, from which it is evident that the catalog size for the final release is order $15$ petabytes.

{\bf MLG: Previous text quoted the final data release version of {\tt Source} catalog to contain $\sim7 \times 10^{12}$ rows, but based on how the text describes it this should be $40 \times 10^9 {\rm objects} \times 80 {\rm visits / year} \times 10 {\rm years} = 32 \times 10^{12} {\rm rows}$? Since it didn't add up I cut out the mention of the final row count, but perhaps my math's just off? Original wording is commented out in the .tex file.}

% The next level would be the full catalogs. This would probably require the specific LSST database server \citedsp{LDM-135} and specific machines and the deployment of the database system and associated subset of data access services (DAX). If we include Object extra (one row per object with e.g. probability distribution it is $\approx 20KB $ per row)  as well as  Object (as above)  extra and Source Catalogs this takes the storage to PBs and begins to require a serious commitment of resources at the proposed DAC. Each object has order 80 observation(sources) per year yielding for the final data release around $7 \times 10^{12} $ rows and forced sources $7 \times 10^{12}$ rows.  The catalog size for the final release is order 15PB.  The evolution of catalog sizes is depicted in \figref{fig:catvol}

\begin{figure}
\begin{center}
\includegraphics[width=0.8\textwidth]{images/CatVolTime}
\caption{Catalog volume over time from \citeds{LDM-144}. \label{fig:catvol}}
\end{center}
\end{figure}

\subsection{An ``{\tt Object Lite}" Catalog}

Many -- perhaps most -- astronomers' science goals will be adequately served by a low-volume subset of the {\tt Object} catalog's columns that do not include, for example, the full posteriors for the bulge+disk likelihood parameters. This {\tt Object Lite} catalog would nominally contain $1840$ bytes per row for the $40 \times 10^{9}$ objects, giving a size of $\approx 7.4 \times 10^{13}$ bytes ($\sim74$ terabytes). Even smaller, science-specific versions of {\tt Object Lite} could be envisioned, with even less columns and/or separate star and galaxy catalogs. These would not be small enough to handle on a laptop, but might be served by a small departmental cluster (a mini-iDAC). Searching even a small {\tt Object Lite} catalog would require some form of database, but many institutes would already have a system which may be capable of loading this data. In this case, LSST might only ship files with documentation and not provide administrative support for the system, but this would allow the {\tt Object Lite} catalog to be widely available to all partner institution iDACs. Distribution options such as peer-to-peer networking to avoid download bandwidth limitations might be possible to implement in this case. 

%Of course searching it would require some form of database, many institutes would already have a system which may be capable of loading this data. We might call this LSST lite, potentially a lot of people might be interested. We would only ship files for this with documentation and no support but it could be made widely available to the collaboration. If bandwidth for download could be a problem we could consider setting up some sort of peer to peer network for distributing it.





\section{Requirements on DACs}
Any institution considering setting up a DAC will need to show commitment on purchasing sufficient storage and CPU power to hold and serve the data.
Sufficient storage here ranges from 100TB minimum to 0.5Exabytes.  Even for the \emph{lite} catalog a set of servers would be needed to make access to the 70 TB feasible. For the full catalog it is order 100 nodes to serve it up.   To serve images a DAC would need some additional servers, depending on load this may be order 10.

The Science Platform is a term used to describe, a Portal, web services and notebooks which  will provide query and analytical tool kits, an open software framework for astronomy and parallel processing, and the associated computational, storage, and communications infrastructure needed to enable science.
It is  described in \citeds{LSE-319} and \citeds{LDM-554}

Deploying the science platform would seem logical with the images and catalogs - again depending on assumed load the platform is modest it requires two servers or so to set up, we suggest then 2 CPUs per simultaneous user (hence if you think you have 200 user but on 50 at a time 100 CPUs).
The open ended question is next tot he data compute - this can be as large as a data center wishes and may want to connect to some local super computer resources.

Having significant hardware above the normal for most astronomy departments would require dedicated technical personnel to set it up and keep it all running.  For the lite Catalogue on existing supported hardware this may not be a significant increase in person power $\iff$ the service is already order 50-100TB. Still one may assume at least a 0.25 FTE for maintaining this (new releases fixes) and more to get it initially set up.  For the full catalog up  one could envisage needed 2FTE. Again there will be software  updates to install as well as new data to load. The full science platform and all the image data would require 1 or 2 storage engineers to mange the large amount of storage and possible one more FTE to keep the Kubernetes or equivalent system updated and the latest software deploys.  Should there be many users local user support will become an specific issue which may not be covered by the usual institutional support requiring further effort. It would seem any potential full DAC without a minimum of 4 FTE should not be considered viable.

\subsection{Policies}
We may require DACs to attach to the LSST authentication system to ensure correct access to the data - this may require some IT work at the hosting site.
There is an open point as to whether  {\em any } qualified user could log in to any DAC. This would facilitate collaborations but would require a single authentication system.

Setting up a DAC will require some level of support from LSST.  The associated DAC support and infrastructure development costs are being separately considered.
%Some required number of PIs would be needed before we could sanction a DAC.

\section{Cost impacts}
There are at least two const impacts of DACs being set up outside USA and Chile. The positive impact is some load may be taken off the existing DACs, the negative is that support will definitely be required to set them up.

We may want to associate specific areas of science to a given iDAC  and encourage the associated users to this center. This creates a customer base for the center, brings together like minded experts, and distributes computing load around the network of centers. It may also enhance internal funding arguments for investment resources by arguing for synergies with local science goals and attracting international users and official endorsement.

% In HEP experiments such as  BABAR various physics analysis groups (science collaborations in LSST ) were assigned to specific international centers as their primary computing and analysis facility, thereby distributing the computing load around the "network". People naturally tend to use the facility with available resources and cycles anyway of course. National groups received credit against their normal "operating common fund" contributions (equivalent to part of the LSST operations cost) based on their local computing contribution, and service to the full collaboration (equivalent to the full LSST data rights community). We have no such system in place for LSST though.

\subsection{Data Transfer}
Even with good networks the data transfer will not be trivial - LSST is not currently set up to distribute data to multiple sites i.e. there is no form of peer to peer sharing. The bandwidth ti NCSA is adequate for Alerts and for receiving data - one could use some day time bandwidth to transfer data to other DACs. A full release does not have to transferred on a given day, if the correct agreements are in place this could b transferred as it is produced.

\subsection{Compute vs Data}
A large cost in the DAC is data storage - if this is set up without a large compute capacity the facility may be less useful  than augmenting an existing DAC to have more compute. One could consider paid for increased compute quota for a given collaboration or set of PIs.
We already have the notion of standard quotas and an allocation committee to adjudicate on bigger proposals.

Another way to approach this would be to have a \emph{Cloud} based DAC where a PI could buy nodes on the provider cloud to access the holdings put there by LSST. In the most radical approach we would move the US DAC to Amazon or Google to achieve this.


\section{Conclusion}
