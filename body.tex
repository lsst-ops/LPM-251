
\section{Introduction}\label{sec:intro}

All access to LSST proprietary data products will be through a Data Access Center (DAC). The United States's DAC will be hosted at the National Center for Supercomputing Applications (NCSA), where authorized users will perform scientific queries and analysis on the full data releases using the LSST Science Platform (LSP). The LSP is now well documented with the vision given in \citeds{LSE-319}, with more formal requirements in \citeds{LDM-554} and the design in \citeds{LDM-542}. The Chilean DAC will be equivalent in functionality to the US DAC, but scaled-down in terms of the computational resources available for query and analysis given the smaller Chilean community \citedsp{LDM-572}. All access to LSST proprietary data and data products is subject to the policies described in \citeds{LSE-349}.

Several LSST partner institutions have expressed interest in hosting the LSST data, in whole or in part, for their affiliated members as an independent Data Access Center (iDAC). In this document we lay out some ideas about what types of data products could be hosted, and the requirements and responsibilities that would be expected of an iDAC hosting LSST proprietary data products.


\section{Types of Data Products for iDACs}\label{sec:data}

The full collection of LSST data products, from images to catalogs, is described by the Data Products Definitions Document \citedsp{LSE-163}. Below we describe three potential levels of data products that iDACs might consider hosting: the full data release with images, the data release catalogs, and a low-volume (``{\tt lite}'') subset of the catalogs.

\subsection{Full Release(s)}

In this case the iDAC would be hosting all of the raw and processed images and catalogs, as described in \citedsp{LSE-163}. Including the raw image data in an iDAC requires roughly $6$ petabytes per year of survey, so this is a serious augmentation on resources in terms of both hardware and personnel. The processed data and associated calibrations bring the total data volume to $0.5$ exabytes for a single data release. Some data volume could be saved by taking only a single calibrated image per band, but the total would still be $60$ petabytes (with compression it may be possible to reduce this even further). Any iDAC considering hosting the full data release should also deploy the full LSST Science Platform \citeds{LSE-319} in order to maximize science productivity and their return on investment in hosting an iDAC.

\subsection{Catalog Server}

Alternatively, an iDAC may find that hosting only the data release catalogs is sufficient for the scientific needs of its community. This will probably require the specific LSST database server \citedsp{LDM-135} and specific machines, and the deployment of the database system and associated subset of data access services (DAX; e.g., web APIs, Qserv, \citeds{LDM-152}). The full {\tt Object} catalog, which contains one row per object with a volume of $\approx 20$ kilobytes per row, is estimated to contain about $40 \times 10^9$ objects (even in the first full-sky data release). Adding to this the full {\tt Source} and {\tt Forced Source} catalogs, which contains one row per measurement in each of the $\sim80$ visit images obtained per year, brings the total storage volume required up into the petabytes range, and will require a serious commitment of resources at the proposed iDAC. The evolution of catalog sizes over the 10-year LSST survey is depicted in \figref{fig:catvol}, from which it is evident that the catalog size for the final release is order $15$ petabytes. For more details on the row counts see the Key Numbers Page\footnote{\url{https://confluence.lsstcorp.org/display/LKB/LSST+Key+Numbers}}.

%{\bf MLG: Previous text quoted the final data release version of {\tt Source} catalog to contain $\sim7 \times 10^{12}$ rows, but based on how the text describes it this should be $40 \times 10^9 {\rm objects} \times 80 {\rm visits / year} \times 10 {\rm years} = 32 \times 10^{12} {\rm rows}$? Since it didn't add up I cut out the mention of the final row count, but perhaps my math's just off? Original wording is commented out in the .tex file.} - WOM: You are correct I simply did not change the force to 20 from 7 !  I added key numbers page which has all of these calculations ..

% The next level would be the full catalogs. This would probably require the specific LSST database server \citedsp{LDM-135} and specific machines and the deployment of the database system and associated subset of data access services (DAX). If we include Object extra (one row per object with e.g. probability distribution it is $\approx 20KB $ per row)  as well as  Object (as above)  extra and Source Catalogs this takes the storage to PBs and begins to require a serious commitment of resources at the proposed DAC. Each object has order 80 observation(sources) per year yielding for the final data release around $7 \times 10^{12} $ rows and forced sources $7 \times 10^{12}$ rows.  The catalog size for the final release is order 15PB.  The evolution of catalog sizes is depicted in \figref{fig:catvol}

\begin{figure}
\begin{center}
\includegraphics[width=0.8\textwidth]{images/CatVolTime}
\caption{Catalog volume over time from \citeds{LDM-144}. \label{fig:catvol}}
\end{center}
\end{figure}

\subsection{An ``{\tt Object Lite}" Catalog}

Many -- perhaps most -- astronomers' science goals will be adequately served by a low-volume subset of the {\tt Object} catalog's columns that do not include, for example, the full posteriors for the bulge+disk likelihood parameters. This {\tt Object Lite} catalog would nominally contain $1840$ bytes per row for the $40 \times 10^{9}$ objects, giving a size of $\approx 7.4 \times 10^{13}$ bytes ($\sim74$ terabytes). Even smaller, science-specific versions of {\tt Object Lite} could be envisioned, with even less columns and/or separate star and galaxy catalogs. These would not be small enough to handle on a laptop, but might be served by a small departmental cluster (a mini-iDAC). Searching even a small {\tt Object Lite} catalog would require some form of database, but many institutes would already have a system which may be capable of loading this data. In this case, LSST might only ship files with documentation and not provide administrative support for the system, but this would allow the {\tt Object Lite} catalog to be widely available to all partner institution iDACs. Distribution options such as peer-to-peer networking to avoid download bandwidth limitations might be possible to implement in this case.

%Of course searching it would require some form of database, many institutes would already have a system which may be capable of loading this data. We might call this LSST lite, potentially a lot of people might be interested. We would only ship files for this with documentation and no support but it could be made widely available to the collaboration. If bandwidth for download could be a problem we could consider setting up some sort of peer to peer network for distributing it.

\section{Requirements and Guidelines for iDACs}\label{sec:reqs}

Since creating, delivering, and supporting the implementation of LSST data products in iDACs creates some cost to the LSST Project, iDACs will be expected to follow some basic requirements and guidelines that are described below. The actual costs of iDAC support and infrastructure development are considered separately in Section \ref{sec:costs}.

\subsection{Data Storage}
Any institution considering setting up an iDAC will need to show commitment on purchasing sufficient storage and CPU power to hold and serve the data. Sufficient storage ranges from $0.5$ exabytes for the full data release(s) down to $100$ terabytes for a catalog server, and potentially further down to $70$ terabytes if the {\tt Object Lite} option is offered. For the full catalog it is order 100 nodes to serve it up, and to serve images a DAC would need some additional servers, depending on load this may be order 10 additional nodes.

\subsection{User Computational Resources}
If the full set of data release products including images and catalogs are desired, it is highly recommended that the iDAC deploy the LSST Science Platform (LSP). The LSP serves as a portal to the data, and provides a user interface of web services and Jupyter notebooks for scientific queries and analysis, an open software framework for astronomy and parallel processing, and the associated computational, storage, and communications infrastructure needed to enable science. The LSP is described in full in \citeds{LSE-319} and \citeds{LDM-554}. Depending on the assumed load, the LSP is relatively modest as it requires only $\sim2$ servers to set up, and it is recommended to have 2 CPUs per simultaneous user (e.g., if the iDAC's desired capability is to serve 200 users, but only expect 50 to be active at a time, then 100 CPUs would be sufficient). From that starting point, the amount of next-to-the-data computational resources can be as large as the data center wishes to provide, and may make use of connecting to e.g., local super computer resources.

\subsection{Dedicated Personnel}
The significant hardware required by an iDAC is above the normal level for most astronomy departments, and would require dedicated technical personnel to set it up and keep it running. For an {\tt Object Lite} catalog running on existing hardware, this might not be a significant increase in person power if the hardware is already serving on order $50$--$100$ terabytes. Still, it is recommended to assume $\gtrsim0.25$ full-time equivalent (FTE) personnel hours for {\tt Object Lite}, and perhaps closer to $\sim2$ FTE for the full catalogs, which includes setting up and maintaining the service, and installing new data releases and software updates every year. For iDACs wishing to host the full data releases' images and catalogs and deploy the LSST Science Platform, it becomes necessary to employ $1$--$2$ storage engineers to mange the large amount of data, and possible one more FTE to keep the Kubernetes (or equivalent) system updated with the latest software deploys. If the iDAC intends to support the science of many local users, support will become a specific issue which may not be covered by the usual institutional funding, and will require further effort. It is therefore recommended that any partner institution wishing to host a full-release iDAC provide a minimum personnel of 5 FTE to be considered viable.

\subsection{Proprietary Data Access Policies}
All iDACs serving the proprietary LSST data products are subject to the policies in \citeds{LSE-349}. It might be necessary for iDACs to use the LSST authentication system to ensure secure access to the data for authorized users only, and this might require some specific IT work at the iDAC host site. It remains an open question whether {\em any} qualified user from any LSST partner institution can log into any iDAC -- this would facilitate collaboration but would also require that iDACs participate in the single authentication system.

\subsection{iDACs Serving Post-Proprietary Data}
{\bf MLG: can we think of any requirements/guidelines for this scenario? Should this document even cover non-partner iDACs? Any non-partner iDAC wishing to serve the two-year-old post-proprietary full release to its users would not, for example, be able to get help installing the LSP. Perhaps we could use this paragraph to further encourage the benefits of partnership.}


\section{Cost Impacts}\label{sec:costs}

As previously mentioned, standing up and maintaining multiple iDACs comes at a significant cost impact to both the LSST Project and the partner institutions. Minimizing these costs -- or at least maximizing the amount of science they enable -- should be at the forefront of all considerations concerning partner iDACs, such as the following propositions.

\subsection{Maximizing Profits with Science-Driven iDACs}
There are two main cost impacts of iDACs being set up outside of the US and Chilean DACs: the positive impact is that some computational load may be taken off of these existing DACs, but the negative impact is the level of support required from the LSST Project in order to get them set up and running. This negative impact could be mitigated by ensuring that science productivity is maximized as a result of this extended effort. One way to do this might be to associate specific areas of science to a given iDAC, and encourage users working in that field to use that iDAC. This could create a customer base for the iDAC, bring together like-minded experts, and effectively distribute the computing load across a network of iDACs. This might also enhance internal funding arguments for investment resources by arguing for synergies with local science goals and attracting international users and official endorsement.

% In HEP experiments such as  BABAR various physics analysis groups (science collaborations in LSST ) were assigned to specific international centers as their primary computing and analysis facility, thereby distributing the computing load around the "network". People naturally tend to use the facility with available resources and cycles anyway of course. National groups received credit against their normal "operating common fund" contributions (equivalent to part of the LSST operations cost) based on their local computing contribution, and service to the full collaboration (equivalent to the full LSST data rights community). We have no such system in place for LSST though.

\subsection{Data Transfer}
Even with good networks the data transfer will not be trivial, and could be quite expensive. LSST is not currently set up to distribute data to multiple sites, i.e., there is no form of peer-to-peer sharing. The bandwidth at NCSA is adequate for receiving data and delivering {\tt Alerts} to brokers during the night; perhaps some day time bandwidth could be used to transfer data to iDACs. A full data release of images and catalogs does not have to transferred within a given day; if the correct agreements are in place with an iDAC, a full release could be transferred slowly as it is produced, and then made available to the iDACs users in whole on the official release day.

\subsection{Compute {\it vs.} Storage Resources}
Data storage is a large cost to iDACs, and could be considered as an overhead relative to the amount of computational resources an iDAC can offer. If an iDAC is set up without a large compute capacity, the facility might be less useful to the science community than e.g., augmenting an existing DAC or iDAC to have more computational resources. It is conceivable that a partner institution may prefer to spend their money increasing the computational quotas available for a given collaboration or set of PIs, and it would be scientifically beneficial if this was possible at all DAC and iDACs. The notion of standard compute quotas and resource allocation committees to adjudicate on large proposals for substantial increases to computational allocations are described in \citeds{LSE-349}. Another way to approach a solution to this issue might be to have a \emph{Cloud}-based iDAC where a user or PI could buy nodes on the provider cloud to access the holdings put there by LSST.
%In the most radical approach we would move the US DAC to Amazon or Google to achieve this.
%{\bf MLG: Re. the last sentence, this would make nCSA worry if they read it? Is it ok to say, it wouldn't be misinterpreted?}

\section{iDAC Proposal and Review Process}

{MLG: is this the right document to outline something like this?}

\section{Conclusion}
